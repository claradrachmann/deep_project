{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "el6WZUx3ctwE",
    "outputId": "f66a38ff-d81d-4b57-d8b2-a43989ee3151"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from google.colab import drive\\ndrive.mount('/content/drive')\\n!ls drive/'My Drive/YYY_deep_project_YYY'\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\"\"\"from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!ls drive/'My Drive/YYY_deep_project_YYY'\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nUc-3FwsYA6X",
    "outputId": "0e1b95cc-1151-4102-ccea-a9e0edbe1f5d"
   },
   "outputs": [],
   "source": [
    "#%cd drive/'My Drive/YYY_deep_project_YYY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OcYm_Z9sdVq4"
   },
   "outputs": [],
   "source": [
    "def get_sequence(infile):\n",
    "\n",
    "    while True:\n",
    "\n",
    "        header = infile.readline()\n",
    "        sequence = infile.readline()\n",
    "\n",
    "        pdb = header[1:5]\n",
    "\n",
    "        if not header or not sequence or set(sequence) == {'X'}:\n",
    "            return\n",
    "        \n",
    "        yield header.strip()[1:], sequence.strip(), pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "mzJSiJbpXkTR",
    "outputId": "2af42cfb-ef10-4316-de19-6b89fa008527"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sequences = []\n",
    "seq_to_pdb = {}\n",
    "count = 0\n",
    "with open('all_heavy.fasta') as infile:\n",
    "\n",
    "        for header, sequence, pdb in get_sequence(infile):\n",
    "            #if count < 500:\n",
    "            sequences.append(list(sequence))\n",
    "                #count += 1\n",
    "            \n",
    "            seq_to_pdb[sequence] = pdb\n",
    "            \n",
    "#sequences = [seq for seq in sequences if len(seq) < 150]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "lengths = [len(seq) for seq in sequences]\n",
    "\n",
    "mode = max(set(lengths), key=lengths.count)\n",
    "\n",
    "print(mode)\n",
    "\n",
    "plt.hist(lengths, bins=100)\n",
    "plt.show()\n",
    "#sequences = [seq for seq in sequences if len(seq) == mode]\n",
    "\n",
    "# Take only shorter sequences to make it more quick\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lVSriKBHdqU7"
   },
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the size of the dataset\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve inputs and targets at the given index\n",
    "        X = self.inputs[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "def create_datasets(sequences, dataset_class, p_train=0.8, p_val=0.1, p_test=0.1):\n",
    "    \n",
    "    \n",
    "    # Define partition sizes\n",
    "    num_train = int(len(sequences)*p_train)\n",
    "    num_val = int(len(sequences)*p_val)\n",
    "    num_test = int(len(sequences)*p_test)\n",
    "\n",
    "    # Split sequences into partitions\n",
    "    sequences_train = sequences[:num_train-1]\n",
    "    sequences_val = sequences[num_train:num_train+num_val-1]\n",
    "    sequences_test = sequences[-num_test:-1]\n",
    "\n",
    "    input_train = [['<sos>'] + list(seq)+['<eos>'] for seq in sequences_train]\n",
    "    input_val = [['<sos>'] + list(seq)+['<eos>'] for seq in sequences_val]\n",
    "    input_test = [['<sos>'] + list(seq)+['<eos>'] for seq in sequences_test]\n",
    "\n",
    "    return (input_train, input_val, input_test)\n",
    "\n",
    "(input_train, input_val, input_test) = create_datasets(sequences, Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YNWaI923d3YE"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "        \n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        \n",
    "        self.embed = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        \n",
    "        self.ff = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        decoder_layers = TransformerDecoderLayer(ninp, nhead, nhid, dropout)\n",
    "        \n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layers, nlayers)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embed.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        self.ff.bias.data.zero_()\n",
    "        self.ff.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        #self.transformer_decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask, seq_lengths):\n",
    "        \n",
    "        # utils.rnn lets you give (B,L,D) tensors where B is the batch size, L is the maxlength, if you use batch_first=True\n",
    "        # Otherwise, give (L,B,D) tensors\n",
    "        #seq_tensor = src.transpose(0,1) # (B,L,D) -> (L,B,D)\n",
    "        \n",
    "        seq_tensor = src\n",
    "        # embed your sequences\n",
    "        seq_tensor = self.embed(seq_tensor) * math.sqrt(self.ninp)\n",
    "        \n",
    "        # pack them up nicely\n",
    "        #packed_input = pack_padded_sequence(seq_tensor, seq_lengths.numpy())\n",
    "        \n",
    "        src = self.pos_encoder(seq_tensor)\n",
    "        \n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "\n",
    "        #output = self.transformer_decoder(output, src_mask)\n",
    "        output = self.ff(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "q4ia5HMZfemk"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qBtnWGt-Al99",
    "outputId": "50cdbe94-9ab1-4a92-a5e6-5f50335a9208"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1, 16, 20,  ...,  6,  3,  2],\n",
       "        [ 1, 16, 20,  ...,  6,  3,  2],\n",
       "        [ 1, 16, 20,  ...,  6,  3,  2],\n",
       "        ...,\n",
       "        [ 1, 18,  8,  ...,  0,  0,  0],\n",
       "        [ 1,  8, 20,  ...,  0,  0,  0],\n",
       "        [ 1, 18, 13,  ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    \n",
    "vocab = ['<pad>', \"<sos>\", \"<eos>\"] + [\"A\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"K\",\"L\",\"M\",\"N\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"V\",\"W\",\"X\",\"Y\"]\n",
    "char_nums = {token:vocab.index(token) for token in vocab}\n",
    "\n",
    "def batchify(data):\n",
    "    \n",
    "    # get the length of each seq in your batch\n",
    "    seq_lengths = torch.LongTensor([len(seq) for seq in data])\n",
    "\n",
    "    train_vectorized = [[char_nums[char] for char in seq] for seq in data]\n",
    "    \n",
    "    # dump padding everywhere, and place seqs on the left.\n",
    "    # NOTE: you only need a tensor as big as your longest sequence\n",
    "    seq_tensor = torch.zeros((len(train_vectorized), seq_lengths.max())).long()\n",
    "    for idx, (seq, seqlen) in enumerate(zip(train_vectorized, seq_lengths)):\n",
    "        seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "\n",
    "    # SORT YOUR TENSORS BY LENGTH!\n",
    "    seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "    seq_tensor = seq_tensor[perm_idx]\n",
    "\n",
    "    return seq_tensor, seq_lengths\n",
    "\n",
    "\n",
    "train_data, train_lengths = batchify(input_train)\n",
    "val_data, val_lengths = batchify(input_train)\n",
    "test_data, test_lengths = batchify(input_train)\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UglAQA33frSw"
   },
   "outputs": [],
   "source": [
    "bptt = 80\n",
    "max_len = max(map(len, sequences))\n",
    "def get_batch(source, i):\n",
    "    n_seqs = min(bptt, len(source) - 1 - i)\n",
    "    data = torch.cat([train_data[i][:-1] for i in range(n_seqs)]).view(n_seqs,max_len+1)\n",
    "    \n",
    "    target = torch.cat([train_data[i][1:] for i in range(n_seqs)]).view(n_seqs,max_len+1).reshape(-1)\n",
    "    \n",
    "    return data, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tLXVBEIrfr-D"
   },
   "outputs": [],
   "source": [
    "ntokens = len(vocab) # the size of vocabulary\n",
    "emsize = 800 # embedding dimension\n",
    "nhid = 400 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 4 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 4 # the number of heads in the multiheadattention models\n",
    "dropout = 0.5 # the dropout value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Qc1m7dgifuPc"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#  ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lr = 5.5 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "#optimizer  = torch.optim.Adam(lr=lr, params=model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "model.to(device)\n",
    "import sys\n",
    "import time\n",
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(vocab)\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        \n",
    "        data, targets = get_batch(train_data, i)\n",
    "    \n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        if data.size(0) != bptt:\n",
    "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "        \n",
    "        output = model(data, src_mask, train_lengths)\n",
    "        \n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        \n",
    "        # Soft, hard accuracy\n",
    "        #o = list(output.view(-1, ntokens)[0])\n",
    "        #t = targets\n",
    "        #print(o,t)\n",
    "        #hard_acc = sum([i for i in range(len(targets)) if o[i] == t[i]])/len(targets)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        log_interval = 1\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}|'.format(\n",
    "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    \n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            if data.size(0) != bptt:\n",
    "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            output = eval_model(data, src_mask, val_lengths)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "tZDramCJfwec",
    "outputId": "6b842e7f-b493-42a1-8118-c263cf475722"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonas/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |     1/   40 batches | lr 5.50 | ms/batch 114145.43 | loss 18.44 | ppl 101859603.57|\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a9671b37e517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m89\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-03421d9305d8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 20 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPTZdip9XkTT"
   },
   "outputs": [],
   "source": [
    "#orch.save(model,'C:/Users/jonas/Desktop/deep_project/models/model.py')\n",
    "# Model class must be defined somewhere\n",
    "#model = torch.load(\"models/model.py\", map_location=torch.device('cpu'))\n",
    "#model.eval()\n",
    "\n",
    "#best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vDPz0G4MfyL6",
    "outputId": "b7e93030-f301-44ea-e6ba-5d917286e439"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  1.87 | test ppl     6.50\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(best_model, test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "id": "ZsUG0R1oXkTT"
   },
   "outputs": [],
   "source": [
    "idx_to_letter = {val:key for key, val in char_nums.items()}\n",
    "\n",
    "def sample_categorical(lnprobs, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Sample an element from a categorical distribution\n",
    "    :param lnprobs: Outcome log-probabilities\n",
    "    :param temperature: Sampling temperature. 1.0 follows the given distribution,\n",
    "        0.0 returns the maximum probability element.\n",
    "    :return: The index of the sampled element.\n",
    "    \"\"\"\n",
    "\n",
    "    if temperature == 0.0:\n",
    "        return lnprobs.argmax()\n",
    "    p = F.softmax(lnprobs / temperature, dim=1)\n",
    "\n",
    "    #print(\"softmaxed probs:\", p)\n",
    "    \n",
    "    return dist.Categorical(p).sample()\n",
    "\n",
    "def sample_sentence(model, query, max_len = 140, temperature=1):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    while len(query) < max_len and '<eos>' not in query:\n",
    "        query_tensor, seq_lengths = batchify([query])\n",
    "        \n",
    "        \n",
    "        src_mask = model.generate_square_subsequent_mask(len(query_tensor)).to(device)\n",
    "        \n",
    "        \n",
    "        output = model(query_tensor, src_mask, torch.Tensor([len(query)])).view(-1, ntokens)\n",
    "        \n",
    "     \n",
    "        next_char_idx = sample_categorical(output, temperature) #0.5\n",
    "     \n",
    "    \n",
    "        try:\n",
    "            query += [idx_to_letter[int(next_char_idx[-1])]]\n",
    "        except IndexError:\n",
    "            query += [idx_to_letter[int(next_char_idx)]]\n",
    "            \n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributions as dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N3rHcGmkG3_m",
    "outputId": "45ef1fc7-ae43-48ac-dc0a-f5382bd3d713"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 E D T Y Y G G G G G G E G G G G G G K N I S L L A V S S L T D T T T Y A A Y F R F T Y Y D N D T Y A M N N S S V S L S L Y C Y Y G G G G G G G G L S G W G G G G V <eos>\n",
      "108 E A T S S T Y A G V G G G G E D T Y T S A A V T S S L S S L S S S S L S S S L S S G G G V S G G G P G S L S S L Y Y A Y Y Y Y T Y Y Y Y Y D T Y Y S S S S S S D T Y D T Y Y Y Y S S N N Q S G G G F T Y Y Y Y Y Y G H <eos>\n",
      "300 E I S L S S S P G V S N R F T I S S S S L T Y G G G G S T Y Y Y S N D N L S S L K N T T Y S S S S S S S S S T S S G G G G G T S S S D T T Y D T Y D T Y Y Y T Y Y Y D T Y F T N W W I N Y Y Y Y Y A A V S S L S L R C A S S G G G G G Y Y Y Y L S V S I D N S S G V W W W F T Y L C A C A V S G G Y C A C A K L S S V S D N A C C T T Y C A S T Y Y Y L S L S S L S K N S S N Y A V Q V T I R F T Q L S L S K P G G G G V A K N Q S S N A D N T Y Y Y G N T Y Y N S P G V T Y Y W F I N Y W A G G S S A V S T Y Y W I S S K N T S S A V Q N Y L S N S N A A F T Y Y Y L Y Y C A G T Y Y A M N A M N S N S L S T Y Y Y S\n",
      "63 E G K N Y D T T Y L S T A P G M N E W W W I S T T Y Y D T I S S S L S S S S S S G G G G S S S G G G G G G S S S S G W W S H <eos>\n",
      "8 E S S G G G H <eos>\n",
      "45 E L S S S L Y Y Y D T V Y Y Q S S L S S V Q V S S S S S I S S S S S S S L R F R C T Y H <eos>\n",
      "167 E W W R C S L S N Y Y F T V Q S S L S L Y Y C A G G Y D T V S T Y P G T Y C A V A A T Y C A G G G S L S L Y C V Q T S S L S S S L R C L S S L L S N Y Y Y Y Y Y Y Y Y Y Y L S S S L Y Y Y Y Y A G G G G G V Q V T Y C A V Q S G G G G G G G G G G K N Y Y Y A V S S S S S T Y S S T T Y Y D N S V S N S T N Y Y Y F R C A V A V S T A G G H <eos>\n",
      "54 E L S V A M N A M N W W W W F T F S N D T Y Y Y Q V A V S S L T N S S S S N A K N S L L K N N D T A A G H <eos>\n",
      "115 E A A A S L C A V S S S S G S S S L S T Y A G G G V S S S L I N V S N Y S S V A M N I N Y Y Y Y K N M N A M N I S L V S S S A G G S G G V T Y W W I Q V S S S S S S L S S G G G G Q V S S S L S S S S L S L S A V R F T Y Y D T V H <eos>\n",
      "43 E G Q V T Y C G G Y G G G G G V S V Q S E L S S L S L S L S L R F T Y D T Y Y Y Y H <eos>\n"
     ]
    }
   ],
   "source": [
    "import torch.distributions as dist\n",
    "ntokens = len(vocab)\n",
    "\n",
    "\n",
    "\n",
    "#dat = batchify(['<sos>'], 1)\n",
    "\n",
    "#print(\" \".join(sample))\n",
    "\n",
    "for _ in range(10):\n",
    "    sample = sample_sentence(model, [\"E\"], max_len = 300, temperature=0.9)\n",
    "\n",
    "    print(len(sample), \" \".join(sample))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VvvgjJGWHhB5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9d0EO5DHXkTT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "project2_jonas.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
