{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "el6WZUx3ctwE",
    "outputId": "f66a38ff-d81d-4b57-d8b2-a43989ee3151"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from google.colab import drive\\ndrive.mount('/content/drive')\\n!ls drive/'My Drive/YYY_deep_project_YYY'\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\"\"\"from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!ls drive/'My Drive/YYY_deep_project_YYY'\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nUc-3FwsYA6X",
    "outputId": "0e1b95cc-1151-4102-ccea-a9e0edbe1f5d"
   },
   "outputs": [],
   "source": [
    "#%cd drive/'My Drive/YYY_deep_project_YYY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OcYm_Z9sdVq4"
   },
   "outputs": [],
   "source": [
    "def get_sequence(infile):\n",
    "\n",
    "    while True:\n",
    "\n",
    "        header = infile.readline()\n",
    "        sequence = infile.readline()\n",
    "\n",
    "        pdb = header[1:5]\n",
    "\n",
    "        if not header or not sequence or set(sequence) == {'X'}:\n",
    "            return\n",
    "        \n",
    "        yield header.strip()[1:], sequence.strip(), pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "mzJSiJbpXkTR",
    "outputId": "2af42cfb-ef10-4316-de19-6b89fa008527"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sequences = []\n",
    "seq_to_pdb = {}\n",
    "count = 0\n",
    "with open('data/sra100k_done.fa') as infile:\n",
    "#with open('data/all_heavy.fasta') as infile:\n",
    "        for header, sequence, pdb in get_sequence(infile):\n",
    "            sequences.append(list(sequence))\n",
    "            seq_to_pdb[sequence] = pdb\n",
    "\n",
    "#sequences = list(filter(lambda x:len(x)<150, sequences))\n",
    "\n",
    "import random\n",
    "random.shuffle(sequences)\n",
    "\n",
    "sequences = sequences[:5000]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "lengths = [len(seq) for seq in sequences]\n",
    "\n",
    "print(len(sequences))\n",
    "\n",
    "plt.hist(lengths, bins=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lVSriKBHdqU7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 500 500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4000, 500, 500)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils import data\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the size of the dataset\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve inputs and targets at the given index\n",
    "        X = self.inputs[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "def create_datasets(sequences, dataset_class, p_train=0.8, p_val=0.1, p_test=0.1):\n",
    "    \n",
    "    \n",
    "    # Define partition sizes\n",
    "    num_train = int(len(sequences)*p_train)\n",
    "    num_val = int(len(sequences)*p_val)\n",
    "    num_test = int(len(sequences)*p_test)\n",
    "    \n",
    "    print(num_train, num_val, num_test)\n",
    "\n",
    "    # Split sequences into partitions\n",
    "    sequences_train = sequences[:num_train]\n",
    "\n",
    "    # add reversed sequences for training and shuffle\n",
    "    #sequences_train += [seq[::-1] for seq in sequences_train]\n",
    "    #shuffle(sequences_train)\n",
    "\n",
    "    sequences_val = sequences[num_train:num_train+num_val]\n",
    "    sequences_test = sequences[num_train+num_val:]\n",
    "\n",
    "    input_train = [['<sos>'] + list(seq)+['<eos>'] for seq in sequences_train]\n",
    "    input_val = [['<sos>'] + list(seq)+['<eos>'] for seq in sequences_val]\n",
    "    input_test = [['<sos>'] + list(seq)+['<eos>'] for seq in sequences_test]\n",
    "\n",
    "    return (input_train, input_val, input_test)\n",
    "\n",
    "(input_train, input_val, input_test) = create_datasets(sequences, Dataset)\n",
    "\n",
    "len(input_train), len(input_val), len(input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YNWaI923d3YE"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "        \n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        \n",
    "        self.t_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        \n",
    "        self.embed = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        \n",
    "        decoder_layers = TransformerDecoderLayer(ninp, nhead, nhid, dropout)\n",
    "        \n",
    "        self.t_decoder = TransformerDecoder(decoder_layers, nlayers)\n",
    "\n",
    "        \n",
    "        self.ff = nn.Linear(ninp, ntoken)\n",
    "        \n",
    "        self._init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float(-1e-10)).masked_fill(mask == 1, float(0.0))\n",
    "        \n",
    "        return mask\n",
    "\n",
    " \n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "       \n",
    "\n",
    "    def forward(self, src, src_mask, tgt, tgt_mask, src_pad_mask, tgt_pad_mask, mem_pad_mask=None):\n",
    "        \n",
    "\n",
    "        embeds = self.embed(src) * math.sqrt(self.ninp)\n",
    "        \n",
    "        positions = self.pos_encoder(embeds)\n",
    "    \n",
    "        \n",
    "        encoded = self.t_encoder(positions)\n",
    "        #encoded = self.t_encoder(positions, src_key_padding_mask=src_pad_mask)\n",
    "        \n",
    "        \n",
    "        if mem_pad_mask is None:\n",
    "            mem_pad_mask = tgt_pad_mask.clone()\n",
    "            \n",
    "        embeds = self.embed(tgt) * math.sqrt(self.ninp)\n",
    "        \n",
    "        positions = self.pos_encoder(embeds)\n",
    "        \n",
    "        #decoded = self.t_decoder(tgt=positions, memory=encoded, tgt_mask=tgt_mask,\n",
    "        #                         tgt_key_padding_mask=tgt_pad_mask,\n",
    "        #                         memory_key_padding_mask=mem_pad_mask)\n",
    "        \n",
    "        decoded = self.t_decoder(tgt=positions, memory=encoded, tgt_mask=tgt_mask)\n",
    "  \n",
    "        output = self.ff(decoded)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qBtnWGt-Al99",
    "outputId": "50cdbe94-9ab1-4a92-a5e6-5f50335a9208"
   },
   "outputs": [],
   "source": [
    "    \n",
    "vocab = ['<pad>', \"<sos>\", \"<eos>\"] + [\"A\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"K\",\"L\",\"M\",\"N\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"V\",\"W\",\"X\",\"Y\"]\n",
    "char_nums = {token:vocab.index(token) for token in vocab}\n",
    "\n",
    "\n",
    "def batchify(data):\n",
    "    \n",
    "    # get the length of each seq in your batch\n",
    "    seq_lengths = torch.LongTensor([len(seq) for seq in data])\n",
    "\n",
    "    train_vectorized = [[char_nums[char] for char in seq] for seq in data]\n",
    "    sources = [seq[:-1] for seq in train_vectorized]\n",
    "    targets = [seq[1:] for seq in train_vectorized]\n",
    "    \n",
    "    # dump padding everywhere, and place seqs on the left.\n",
    "    # NOTE: you only need a tensor as big as your longest sequence\n",
    "    src_tensor = torch.zeros((len(train_vectorized), seq_lengths.max()-1)).long()\n",
    "    tgt_tensor = torch.zeros((len(train_vectorized), seq_lengths.max()-1)).long()\n",
    "\n",
    "    \n",
    "    for idx, (seq, seqlen) in enumerate(zip(sources, seq_lengths)):\n",
    "        src_tensor[idx, :(seqlen-1)] = torch.LongTensor(seq)\n",
    "    \n",
    "    for idx, (seq, seqlen) in enumerate(zip(targets, seq_lengths)):\n",
    "        tgt_tensor[idx, :(seqlen-1)] = torch.LongTensor(seq)\n",
    "        \n",
    "        \n",
    "    # SORT YOUR TENSORS BY LENGTH!\n",
    "    seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "    \n",
    "    src_tensor = src_tensor[perm_idx]\n",
    "    tgt_tensor = tgt_tensor[perm_idx]\n",
    "\n",
    "    return src_tensor, tgt_tensor\n",
    "\n",
    "\n",
    "train_src, train_tgt = batchify(input_train)\n",
    "val_src, val_tgt = batchify(input_val)\n",
    "test_src, test_tgt = batchify(input_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "UglAQA33frSw"
   },
   "outputs": [],
   "source": [
    "bptt = 100\n",
    "\n",
    "def get_batch(sources, targets, i):\n",
    "    n_seqs = min(bptt, len(sources) - 1 - i)\n",
    "    \n",
    "    max_len = max([len(seq) for seq in sources])\n",
    "    \n",
    "    src = torch.cat([sources[i] for i in range(n_seqs)]).view(n_seqs,max_len)\n",
    "        \n",
    "    target = torch.cat([targets[i] for i in range(n_seqs)]).view(n_seqs,max_len)#.reshape(-1)\n",
    "    \n",
    "    return src, target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tLXVBEIrfr-D"
   },
   "outputs": [],
   "source": [
    "ntokens = len(vocab) # the size of vocabulary\n",
    "emsize = 400 # embedding dimension\n",
    "nhid = 400 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 4 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 4 # the number of heads in the multiheadattention models\n",
    "dropout = 0.5 # the dropout value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1,  3, 18,  ..., 20, 16,  8],\n",
       "         [ 1,  3, 18,  ..., 20, 16,  8],\n",
       "         [ 1,  8,  6,  ..., 20, 16,  8],\n",
       "         ...,\n",
       "         [ 1,  8,  6,  ...,  0,  0,  0],\n",
       "         [ 1,  8,  6,  ...,  0,  0,  0],\n",
       "         [ 1,  3, 18,  ...,  0,  0,  0]]),\n",
       " tensor([[ 3, 18, 20,  ..., 16,  8,  2],\n",
       "         [ 3, 18, 20,  ..., 16,  8,  2],\n",
       "         [ 8,  6, 18,  ..., 16,  8,  2],\n",
       "         ...,\n",
       "         [ 8,  6, 18,  ...,  0,  0,  0],\n",
       "         [ 8,  6, 18,  ...,  0,  0,  0],\n",
       "         [ 3, 18, 20,  ...,  0,  0,  0]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_src, train_tgt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Qc1m7dgifuPc"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#  ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lr = 0.1 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "#optimizer  = torch.optim.Adam(lr=lr, params=model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.8)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "model.to(device)\n",
    "import sys\n",
    "import time\n",
    "def train():\n",
    "    \n",
    "    train_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(vocab)\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    \n",
    "    for batch, i in enumerate(range(0, train_src.size(0) - 1, bptt)):\n",
    "        \n",
    "        data, targets = get_batch(train_src, train_tgt, i)\n",
    "    \n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        \n",
    "        targets = targets.view(data.shape)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "            \n",
    "        src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "        src_pad_mask = (data == 0).bool().view(data.size(1), data.size(0))\n",
    "        \n",
    "        tgt_mask = model.generate_square_subsequent_mask(targets.size(0)).to(device)\n",
    "        tgt_pad_mask = (targets == 0).bool().view(targets.size(1), targets.size(0))\n",
    "        \n",
    "        output = model(data, src_mask, targets, tgt_mask, src_pad_mask, tgt_pad_mask)\n",
    "     \n",
    "        output_trans = output.view(-1, ntokens)\n",
    "        #target_trans = targets.view(-1, targets.size(0) * targets.size(1)).squeeze(0)\n",
    "        target_trans = targets.view(-1)\n",
    "            \n",
    "        loss = criterion(output_trans, target_trans)\n",
    "        \n",
    "        n_batches += 1\n",
    "        train_loss += loss\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.4)\n",
    "        \n",
    "        # Soft, hard accuracy\n",
    "        #o = list(output.view(-1, ntokens)[0])\n",
    "        #t = targets\n",
    "        #print(o,t)\n",
    "        #hard_acc = sum([i for i in range(len(targets)) if o[i] == t[i]])/len(targets)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        log_interval = 10\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}|'.format(\n",
    "                    epoch, batch, len(train_src) // bptt, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "    \n",
    "    train_losses.append(train_loss/n_batches)\n",
    "\n",
    "def evaluate(eval_model, src, tgt):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, src.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(src, tgt, i)\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            \n",
    "            targets = targets.view(data.shape)\n",
    "            \n",
    "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            src_pad_mask = (data == 0).bool().view(data.size(1), data.size(0))\n",
    "        \n",
    "            tgt_mask = model.generate_square_subsequent_mask(targets.size(0)).to(device)\n",
    "            tgt_pad_mask = (targets == 0).bool().view(targets.size(1), targets.size(0))    \n",
    "            \n",
    "            output = eval_model(data, src_mask, targets, tgt_mask, src_pad_mask, tgt_pad_mask)\n",
    "            \n",
    "            output_trans = output.view(-1, ntokens)\n",
    "            #target_trans = targets.view(-1, targets.size(1) * targets.size(0)).squeeze(0)\n",
    "            target_trans = targets.view(-1)\n",
    "            \n",
    "            total_loss += len(data) * criterion(output_trans, target_trans).item()\n",
    "            \n",
    "    return total_loss / (len(src) - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "tZDramCJfwec",
    "outputId": "6b842e7f-b493-42a1-8118-c263cf475722"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\miniconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    10/   40 batches | lr 0.10 | ms/batch 529.29 | loss  3.31 | ppl    27.36|\n",
      "| epoch   1 |    20/   40 batches | lr 0.10 | ms/batch 410.59 | loss  2.66 | ppl    14.25|\n",
      "| epoch   1 |    30/   40 batches | lr 0.10 | ms/batch 409.39 | loss  2.50 | ppl    12.19|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 17.90s | valid loss  2.45 | valid ppl    11.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |    10/   40 batches | lr 0.09 | ms/batch 451.80 | loss  2.52 | ppl    12.37|\n",
      "| epoch   2 |    20/   40 batches | lr 0.09 | ms/batch 416.09 | loss  2.20 | ppl     8.99|\n",
      "| epoch   2 |    30/   40 batches | lr 0.09 | ms/batch 415.69 | loss  2.13 | ppl     8.44|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 17.33s | valid loss  2.25 | valid ppl     9.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |    10/   40 batches | lr 0.09 | ms/batch 480.91 | loss  2.21 | ppl     9.13|\n",
      "| epoch   3 |    20/   40 batches | lr 0.09 | ms/batch 418.29 | loss  1.95 | ppl     7.06|\n",
      "| epoch   3 |    30/   40 batches | lr 0.09 | ms/batch 416.59 | loss  1.91 | ppl     6.72|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 17.54s | valid loss  2.17 | valid ppl     8.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |    10/   40 batches | lr 0.08 | ms/batch 452.70 | loss  2.02 | ppl     7.55|\n",
      "| epoch   4 |    20/   40 batches | lr 0.08 | ms/batch 415.49 | loss  1.80 | ppl     6.05|\n",
      "| epoch   4 |    30/   40 batches | lr 0.08 | ms/batch 412.19 | loss  1.77 | ppl     5.84|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 17.20s | valid loss  2.07 | valid ppl     7.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |    10/   40 batches | lr 0.08 | ms/batch 454.20 | loss  1.88 | ppl     6.56|\n",
      "| epoch   5 |    20/   40 batches | lr 0.08 | ms/batch 416.99 | loss  1.68 | ppl     5.38|\n",
      "| epoch   5 |    30/   40 batches | lr 0.08 | ms/batch 412.09 | loss  1.66 | ppl     5.24|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 17.21s | valid loss  2.01 | valid ppl     7.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |    10/   40 batches | lr 0.07 | ms/batch 460.90 | loss  1.77 | ppl     5.89|\n",
      "| epoch   6 |    20/   40 batches | lr 0.07 | ms/batch 414.19 | loss  1.59 | ppl     4.89|\n",
      "| epoch   6 |    30/   40 batches | lr 0.07 | ms/batch 418.59 | loss  1.57 | ppl     4.80|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 17.36s | valid loss  1.97 | valid ppl     7.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |    10/   40 batches | lr 0.07 | ms/batch 463.00 | loss  1.68 | ppl     5.39|\n",
      "| epoch   7 |    20/   40 batches | lr 0.07 | ms/batch 418.09 | loss  1.51 | ppl     4.53|\n",
      "| epoch   7 |    30/   40 batches | lr 0.07 | ms/batch 414.29 | loss  1.49 | ppl     4.45|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 17.38s | valid loss  1.90 | valid ppl     6.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |    10/   40 batches | lr 0.07 | ms/batch 455.00 | loss  1.60 | ppl     4.96|\n",
      "| epoch   8 |    20/   40 batches | lr 0.07 | ms/batch 418.49 | loss  1.44 | ppl     4.20|\n",
      "| epoch   8 |    30/   40 batches | lr 0.07 | ms/batch 414.19 | loss  1.42 | ppl     4.15|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 17.40s | valid loss  1.83 | valid ppl     6.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |    10/   40 batches | lr 0.06 | ms/batch 455.20 | loss  1.52 | ppl     4.59|\n",
      "| epoch   9 |    20/   40 batches | lr 0.06 | ms/batch 414.09 | loss  1.37 | ppl     3.93|\n",
      "| epoch   9 |    30/   40 batches | lr 0.06 | ms/batch 418.29 | loss  1.35 | ppl     3.86|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 17.27s | valid loss  1.77 | valid ppl     5.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |    10/   40 batches | lr 0.06 | ms/batch 457.00 | loss  1.45 | ppl     4.26|\n",
      "| epoch  10 |    20/   40 batches | lr 0.06 | ms/batch 414.39 | loss  1.30 | ppl     3.67|\n",
      "| epoch  10 |    30/   40 batches | lr 0.06 | ms/batch 418.79 | loss  1.28 | ppl     3.61|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 17.30s | valid loss  1.72 | valid ppl     5.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |    10/   40 batches | lr 0.06 | ms/batch 455.70 | loss  1.38 | ppl     3.96|\n",
      "| epoch  11 |    20/   40 batches | lr 0.06 | ms/batch 420.69 | loss  1.24 | ppl     3.44|\n",
      "| epoch  11 |    30/   40 batches | lr 0.06 | ms/batch 414.09 | loss  1.22 | ppl     3.38|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 17.29s | valid loss  1.64 | valid ppl     5.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |    10/   40 batches | lr 0.05 | ms/batch 456.10 | loss  1.30 | ppl     3.67|\n",
      "| epoch  12 |    20/   40 batches | lr 0.05 | ms/batch 418.49 | loss  1.17 | ppl     3.21|\n",
      "| epoch  12 |    30/   40 batches | lr 0.05 | ms/batch 414.19 | loss  1.15 | ppl     3.17|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 17.30s | valid loss  1.58 | valid ppl     4.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |    10/   40 batches | lr 0.05 | ms/batch 460.20 | loss  1.23 | ppl     3.43|\n",
      "| epoch  13 |    20/   40 batches | lr 0.05 | ms/batch 415.09 | loss  1.10 | ppl     3.01|\n",
      "| epoch  13 |    30/   40 batches | lr 0.05 | ms/batch 415.19 | loss  1.09 | ppl     2.97|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 17.30s | valid loss  1.50 | valid ppl     4.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |    10/   40 batches | lr 0.05 | ms/batch 464.00 | loss  1.16 | ppl     3.20|\n",
      "| epoch  14 |    20/   40 batches | lr 0.05 | ms/batch 417.89 | loss  1.04 | ppl     2.83|\n",
      "| epoch  14 |    30/   40 batches | lr 0.05 | ms/batch 414.39 | loss  1.03 | ppl     2.79|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 17.40s | valid loss  1.42 | valid ppl     4.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |    10/   40 batches | lr 0.05 | ms/batch 456.00 | loss  1.10 | ppl     3.00|\n",
      "| epoch  15 |    20/   40 batches | lr 0.05 | ms/batch 414.19 | loss  0.98 | ppl     2.68|\n",
      "| epoch  15 |    30/   40 batches | lr 0.05 | ms/batch 414.69 | loss  0.97 | ppl     2.63|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 17.29s | valid loss  1.36 | valid ppl     3.88\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 30 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(model, val_src, val_tgt)\n",
    "    val_losses.append(val_loss)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "XPTZdip9XkTT"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAA3aUlEQVR4nO3deXiV1bn38e+dgSGMIQyZCSACmUhCGBRFKA6gIDIpiFrtUd46tfa0PXo62facDu/b1qOe1nmoWgSZRZxQC6JVGRIgBMIMgSSEIZAQSEKm+/3j2SJiEoLsnSfZuT/XlcvsZ9j7BoFf1lrPWktUFWOMMeZcAW4XYIwxpnmygDDGGFMnCwhjjDF1soAwxhhTJwsIY4wxdQpyuwBv6t69u8bFxbldhjHGtBgZGRlHVbVHXef8KiDi4uJYv36922UYY0yLISK59Z2zLiZjjDF1soAwxhhTJwsIY4wxdfKrMQhjjP+oqqoiLy+PiooKt0vxC+3atSM6Oprg4OBG32MBYYxplvLy8ujUqRNxcXGIiNvltGiqSlFREXl5efTp06fR91kXkzGmWaqoqCAsLMzCwQtEhLCwsAtujVlAGGOaLQsH7/k2v5etPiAqqmp4fvUePtt91O1SjDGmWWn1AREUIDz/yR5e+nSv26UYY5qR4uJinnrqqQu+7/rrr6e4uNj7BbnAAiIwgKlDovnntsMcOmFPSxhjHPUFRE1NTYP3vfPOO3Tt2tVHVTWtVh8QADenx1CrsDAjz+1SjDHNxCOPPMLu3btJSUlh6NChjBkzhltvvZWkpCQAbrrpJoYMGUJCQgLPPffcmfvi4uI4evQo+/btY9CgQdxzzz0kJCRw7bXXUl5e7tYv51uxx1yBPt07MKxPNxasP8B9o/vZwJgxzcxv3trC1oITXn3P+MjOPDoxod7zf/zjH8nOzmbjxo2sWrWKG264gezs7DOPib700kt069aN8vJyhg4dytSpUwkLC/vae+zcuZO5c+fy/PPPc/PNN7No0SJuu+02r/46fMlaEB63pMewr6iMNXuPuV2KMaYZGjZs2NfmEDz55JMMHjyYESNGcODAAXbu3PmNe/r06UNKSgoAQ4YMYd++fU1UrXdYC8Lj+qQIfr1sC/PXHWBE37Dz32CMaTIN/aTfVDp06HDm+1WrVvHhhx/y+eefExISwujRo+ucY9C2bdsz3wcGBra4LiZrQXi0bxPIxJRI3sk+yImKKrfLMca4rFOnTpSWltZ5rqSkhNDQUEJCQti2bRtffPFFE1fXNCwgznJLegwVVbUs21jgdinGGJeFhYUxcuRIEhMT+elPf/q1c+PGjaO6uprk5GR++ctfMmLECJeq9C1RVbdr8Jr09HS9mA2DVJXxT3xCm6AAlj1whRcrM8ZcqJycHAYNGuR2GX6lrt9TEclQ1fS6rrcWxFlEhJvTY8jKKyHnoHefmDDGmJbGAuIck1OjaBMYwBvrDrhdijHGuMoC4hyhHdpwTUIvlm7M53R1wzMmjTHGn1lA1OGW9BiKy6pYseWQ26UYY4xrLCDqcMUl3Ynq2p75662byRjTellA1CEgQJg2JJpPdx0l73iZ2+UYY4wrfBYQIhIjIitFJEdEtojID+u4ZrSIlIjIRs/Xr846N05EtovILhF5xFd11md6ejQAC9bbAn7GmPPr2LEjAAUFBUybNq3Oa0aPHs35HsV//PHHKSv76gdTN5cP92ULohr4saoOAkYA94tIfB3XfaKqKZ6v3wKISCDwN2A8EA/MrOden4kODeGKS7qzMCOPmlr/mStijPGtyMhIFi5c+K3vPzcg3Fw+3GcBoaoHVTXT830pkANENfL2YcAuVd2jqpXAPGCSbyqt3y1DY8gvLudfu2y3OWNam4cffvhr+0H8+te/5je/+Q1jx44lLS2NpKQk3nzzzW/ct2/fPhITEwEoLy9nxowZJCcnc8stt3xtLaZ7772X9PR0EhISePTRRwFnAcCCggLGjBnDmDFjgK+WDwd47LHHSExMJDExkccff/zM5/lqWfEmWaxPROKAVGBNHacvE5FNQAHwE1XdghMkZ48Q5wHD63nv2cBsgNjYWC9WDdfE9yI0JJg31h9g1KU9vPrexpgL8O4jULjZu+8ZngTj/1jv6RkzZvDQQw9x3333ATB//nzee+89fvSjH9G5c2eOHj3KiBEjuPHGG+vdIuDpp58mJCSErKwssrKySEtLO3Pud7/7Hd26daOmpoaxY8eSlZXFD37wAx577DFWrlxJ9+7dv/ZeGRkZvPzyy6xZswZVZfjw4Vx11VWEhob6bFlxnw9Si0hHYBHwkKqeOz05E+itqoOB/wWWfnlbHW9VZz+Pqj6nqumqmt6jh3f/EW8bFMhNqVF8sOUQx09VevW9jTHNW2pqKocPH6agoIBNmzYRGhpKREQEP/vZz0hOTubqq68mPz+fQ4fqfxx+9erVZ/6hTk5OJjk5+cy5+fPnk5aWRmpqKlu2bGHr1q0N1vPpp58yefJkOnToQMeOHZkyZQqffPIJ4LtlxX3aghCRYJxwmKOqi889f3ZgqOo7IvKUiHTHaTHEnHVpNE4Lo8ndMjSGl/+1jyUb8vneFX3Of4Mxxvsa+Enfl6ZNm8bChQspLCxkxowZzJkzhyNHjpCRkUFwcDBxcXF1LvN9trpaF3v37uXPf/4z69atIzQ0lDvvvPO879PQunm+Wlbcl08xCfAikKOqj9VzTbjnOkRkmKeeImAd0F9E+ohIG2AGsMxXtTZkYHhnBkd3Yf76Aw3+DzLG+J8ZM2Ywb948Fi5cyLRp0ygpKaFnz54EBwezcuVKcnNzG7x/1KhRzJkzB4Ds7GyysrIAOHHiBB06dKBLly4cOnSId99998w99S0zPmrUKJYuXUpZWRmnTp1iyZIlXHnllV781X6TL1sQI4Hbgc0istFz7GdALICqPgNMA+4VkWqgHJihzr/C1SLyAPA+EAi85BmbcMXNQ2P4+ZJssvJKGBzT1a0yjDFNLCEhgdLSUqKiooiIiGDWrFlMnDiR9PR0UlJSGDhwYIP333vvvdx1110kJyeTkpLCsGHDABg8eDCpqakkJCTQt29fRo4ceeae2bNnM378eCIiIli5cuWZ42lpadx5551n3uPuu+8mNTXVp7vU2XLfAHtXQ69ECOlW5+kTFVUM+92HTE6N5g9Tki6ySmNMY9hy395ny31fqLJjMHcmzJkOp0/WeUnndsFcnxTBW5sKKKusbuICjTHGHRYQId1g8rNQsAHm3QrVp+u87Jb0GE6eruadzYVNXKAxxrjDAgJg0ASY9DfY+zEs/B7UfLOVMKxPN/p078B82yfCmCbjT13gbvs2v5cWEF9KmQnj/i9sWw5v/QBqa792WkSYnh7N2n3H2HOk7q4oY4z3tGvXjqKiIgsJL1BVioqKaNeu3QXd1yQzqVuMEd+HimJY9Qdo1wWu+z2c9QzztLRo/rJiB/PX5/HI+IafXjDGXJzo6Gjy8vI4cuSI26X4hXbt2hEdHX1B91hAnOuqh6G8GL54CtqHwlX/ceZUz87tGDOgB4sy8/jJtZcSFGgNMGN8JTg4mD59bHKqm+xfuHOJOC2HlFmw8new5tmvnb45PYYjpadZud1+qjHG+DcLiLoEBMDEJ2HgBHj3P2DTvDOnxgzsSfeObXnDBquNMX7OAqI+gUEw9UXoMwqW3gfb3gYgODCAqUOiWLn9MIdPNLx2ijHGtGQWEA0JbgczXofIFFhwlzPjGqebqaZWWZSZ7259xhjjQxYQ59O2E8xaCN36OjOu8zLo16MjQ+NCWWAL+Blj/JgFRGOEdIPbl0CH7jBnKhzO4eb0GPYcPcW6fcfdrs4YY3zCAqKxOkfA7UshsC28NpkJsZV0bBtkg9XGGL9lAXEhuvVxWhJV5bSfO4Vb44N5Z/NBSiuq3K7MGGO8zgLiQvWKh9sWwckj/KjwEdpUlfDWpoNuV2WMMV5nAfFtRKfDzNdpd2Iv8zr8hTfX7XC7ImOM8ToLiG+r72hk2ssMqNnJg4d+xY78o25XZIwxXmUBcTEGTaBs/BNcEbiFmgV1LxNujDEtlQXEReo4/A4WdL+fQcUfU/PmA99YJtwYY1oqCwgv6Hntj3i8egqBWXNhxc/BJs8ZY/yABYQXXHFJd+aHzOL9jjc5y4Sv/pPbJRljzEXzWUCISIyIrBSRHBHZIiI/rOOaWSKS5fn6TEQGn3Vun4hsFpGNIrLeV3V6Q2CAMG1oLPcWTePUoJvrXCbcGGNaGl+2IKqBH6vqIGAEcL+IxJ9zzV7gKlVNBv4LeO6c82NUNUVV031Yp1dMHxKNEsAL3X5U5zLhxhjT0vgsIFT1oKpmer4vBXKAqHOu+UxVv1zM6AvgwvbDa0ZiuoUwsl935mcUUjvlha+WCX/zATi01e3yjDHmgjXJGISIxAGpwJoGLvs34N2zXiuwQkQyRGR2A+89W0TWi8h6t/euvXloDPnF5fwr96SzTPiQO2HzQnj6Mnj1Jtixwp5yMsa0GD4PCBHpCCwCHlLVE/VcMwYnIB4+6/BIVU0DxuN0T42q615VfU5V01U1vUePHl6u/sJcG9+LLu2DnQX82naCCY/Bv2+Fsb+CI9vg9enwt2Gw7kWoPOVqrcYYcz4+DQgRCcYJhzmquriea5KBF4BJqlr05XFVLfD89zCwBBjmy1q9oV1wIJNTo1ix5RDHT1U6B0O6wZU/hoc2w5QXoG1HePvf4bF4+PDXcKLA1ZqNMaY+vnyKSYAXgRxVfayea2KBxcDtqrrjrOMdRKTTl98D1wLZvqrVm25Oj6GyppalG8/ZbS4wGJKnwz0r4a73nDGKfz0BjyfBwn+D/Ax3CjbGmHoE+fC9RwK3A5tFZKPn2M+AWABVfQb4FRAGPOXkCdWeJ5Z6AUs8x4KA11X1PR/W6jXxkZ1JiurCG+sOcOflcXh+DV8Rgd6XOV/H98Ha5yHzVcheCDHDYcR9zlNQgb78X2OMMecn/rRlZnp6uq5f7/6Uide+yOWXS7NZ9sBIkqO7nv+GihOwcQ588TQU50KXWBg+G9LugHZdfF6vMab1EpGM+qYS2ExqH7hxcCRtgwIav9tcu84w4l74wQa4ZQ50jYEVv3DGKd59GI7t8W3BxhhTBwsIH+jSPpjrkyJYtrGA8sqaxt8YEAiDJsBd78Dsj52upnUvwpNpMPdW2PeprfNkjGkyFhA+MnNYLKWnq3n8w2+5mVBkCkx51nn66cofw/7P4e83wLOjYNMbUHsBwWOMMd+CBYSPDOvTjdtGxPLs6j28l1347d+ocwSM/aUzn2LiE1BTCUtmw7NXwe6V3ivYGGPOYQHhQ7+cEM/gmK78ZMEm9hw5eXFvFtzemZl93xcw7SU4XQKv3QRzpsPhbd4o1xhjvsYCwofaBgXy1Kw0ggOFe/+RSVmlF3acE4HEqXD/Orjmt7D/C3j6clj+Izjp7lIjxhj/YgHhY1Fd2/PkzFR2HC7lZ4s347XHioPbwcgfwg82wtB/g4xX4MlU+OQvUFXunc8wxrRqFhBN4Mr+Pfj3qy9l6cYC/vFFrnffvEMYXP8nuH8N9LkSPvot/HUoZM23hQGNMRfFAqKJ3D/mEr4zsCe/Xb6VzP3Hz3/DhereH2bOhe++5az/tPgeeGEs5H7m/c8yxrQKFhBNJCBA+J+bUwjv0o7752RSdPK0bz6ozyi4ZxXc9AyUFsLL42HeLCja7ZvPM8b4LQuIJtQlJJinZw2h6FQlP5y3kZpaH016CwiAlJnwYAaM+YXzOOzfhsG7j0DZMd98pjHG71hANLHEqC7896REPt11lMc+2O7bD2sTAlf91FnCI2UWrH0WnkyBz/4K1T5qwRhj/IYFhAtuHhrDjKEx/G3lbj7cesj3H9ipF9z4JHz/XxA9FFb83GlRbFlqS3cYY+plAeGSX9+YQGJUZ340fyO5RU20u1yveLhtEdy2GIJDYMF34aVxkOf+CrjGmObHAsIl7YIDeXrWEAJE+P4/MqmoasK1lS4ZC9//FCY+Ccf3Ok87LbgLju5suhqMMc2eBYSLYrqF8PgtKWwrPMEvlmZ7bxJdYwQEwpDvwoOZMOo/YPu78Nd0eHokfPz/bPkOY4xtGNQcPPbBDp78aCe/n5zErcNj3SmitBCyF8HWZXBgDaDQ/VIYdCPE3wjhyc4yH8YYv9LQhkEWEM1ATa1y58trWbPnGAvvvaxxu9D5Umkh5LwFOcs8e1DUQmgcDJoIgyZB1BDnUVpjTItnAdECHDtVycT//RSA5Q9eQWiHNi5X5HGqCLa/7bQs9qyC2iroFOmERfyNEHuZ011ljGmRLCBaiE0Hipn+zOeM6BfGy3cOJTCgmXXplBfDjvdh65uw+yOoroAOPZyd7+JvhLgrITDY7SqNMRfAlT2pRSRGRFaKSI6IbBGRH9ZxjYjIkyKyS0SyRCTtrHPjRGS759wjvqqzORkc05VHb4xn9Y4jPPlRM3yiqH1XGHwLzHwdfrobpr0McVc4CwO+Nhn+dAksvQ+2v2cT8YzxA0E+fO9q4MeqmikinYAMEflAVbeedc14oL/nazjwNDBcRAKBvwHXAHnAOhFZds69funWYbFk5hbz5D93khLblTEDerpdUt3adoTEKc5XVTns/qfTDZWzHDbOgTad4NLrvmpZhHRzu2JjzAXyWUCo6kHgoOf7UhHJAaKAs/+RnwS8qk4/1xci0lVEIoA4YJeq7gEQkXmea/0+IESE/74pkS0FJTw0byPLH7yCmG4hbpfVsOD2MPAG56u6Evauhpw3YdvbkL3QuSa0D0SlQWSa89/wZCdkjDHNli9bEGeISByQCqw551QUcOCs13meY3UdH17Pe88GZgPExrr0iKiXtW8TyDO3DWHiXz/lvjmZLPj+ZbQLbiEDwUFtoP/VztcN/wMHvoADa6EgE/avcR6lBZAA6D7AExqpTnCEJ0JQW3frN8ac4fOAEJGOwCLgIVU9ce7pOm7RBo5/86Dqc8Bz4AxSX0SpzUpc9w48dnMK97y6nt+8tYU/TEl2u6QLFxjkjFHEXfHVsZOHoWAD5Gc6obHjfadLCiAgGHolfD00egx03scY0+R8+jdPRIJxwmGOqi6u45I8IOas19FAAdCmnuOtyjXxvbhvdD+eWrWb1NhQbk6POf9NzV3Hns7YxKXXOa9VoSTPCYsvQ2PzQlj/knM+OMTpjopM/aqLqltfm4dhTBPwWUCIiAAvAjmq+lg9ly0DHvCMMQwHSlT1oIgcAfqLSB8gH5gB3OqrWpuzf7/mUjYeKOaXS7OJj+hMYlQXt0vyLhHoGuN8xU9yjtXWwrE9Tlh82drI+Dusedo537aLExYDb4CEydChu2vlG+PPfDYPQkSuAD4BNgNfbo78MyAWQFWf8YTIX4FxQBlwl6qu99x/PfA4EAi8pKq/O99ntvR5EPU5evI0E578lOAgYfkDV9IlpBXONaiphiPbnMAoyIR9/4Kj20ECod93IGk6DLwe2nZyu1JjWhSbKOcHMnKPM+O5z7myfw9euCOdgOY2ic4Nh7bA5gWweRGU7Ieg9jBgnBMWl1xtA97GNIIFhJ945bN9PLpsCz++5lIeHNvf7XKaD1XnSanNC2DLEig7Cu26OAsNJk1z5mHYciDG1KmhgLDHQ1qQOy7rTeb+4/zlgx1U1dTy0NWXWksCnHGM2OHO17g/wt5VzkD3liWw4TXoGO6Z1DfNGbuwVWmNaRRrQbQwp6tr+OXSbOavz+Pa+F78zy0pdGhrOV+nqnLnMdrNC2DnCqipdCbsJU13WhY9BrhdoTGusy4mP6OqvPyvffz321u5tFcnnr8jvfnPtnZbeTFsW+6Exd7VzhLm4UlOWCROhS7RbldojCssIPzU6h1HeOD1TIICA3hqVhoj+oa5XVLLUFrodD9tXgj5nj8vsZdD0lSInwwd7PfRtB4WEH5sz5GT3P3qevYXlfGbSQnMGt7b7ZJalmN7nKegNi9wHpsNCIYB4yH1dmfvbhvcNn7OAsLPnaio4gdzN7Bq+xFuH9GbX02MJzjQZhpfEFUo3AxZb8CmuVBW5GyMlDITUmZBWD+3KzTGJywgWoGaWuX/vreN51bv4bK+YTw1K6357ErX0lRXwo73nCegdn3ojFf0vgJSb3Nme7ex8R7jPywgWpFFGXn85+LN9OrSlhfuGMqAcJtZfFFOFMDG12HDP+D4Xmefi6SpThdU1BB7ZNa0eBYQrUzm/uP8n9cyKDtdzeMzUrkmvpfbJbV8qpD7mdOq2LIUqsuhxyCnVTF4hq0HZVqsi95yVER+KCKdPVuEvigimSJyrXfLNN6SFhvKsgdG0rdHR2a/tp6/rdyFP/0g4AoRiBsJk5+Bn+yAiU9Amw6w4ufwlwHwxm3OnIuaarcrNcZrGtWCEJFNqjpYRK4D7gd+CbysqmnnubVJWQvi6yqqaviPhVks21TAxMGR/L+pybRvY0/leNXhHKf7adM8Z4mPThEweKbTsrCBbdMCXHQXk4hkqWqyiDwBrFLVJSKyQVVTvV3sxbCA+CZV5emPd/On97eTGNmF5+4YQkSX9m6X5X+qK2Hn+5D5Guz6wDOwPdIZq4i/0WltGNMMeSMgXsbZBrQPMBhnCe5VqjrEm4VeLAuI+n249RA/nLeBkLZBPHv7ENJiQ90uyX+dOOg8KrvhH3BstzOwnXCTM2s77gqbW2GaFW8ERACQAuxR1WIR6QZEq2qWVyu9SBYQDdtxqJS7X1lPYUkFf5iSxNQhtryET6nC/s+dVkXOMqg86Vk4cKqzFlRkqj0FZVznjYAYCWxU1VMichuQBjyhqrneLfXiWECc3/FTldw3J5PP9xQxe1RfHh43kEBbEdb3KsucLqjNC79aOLBbv68WDuxuy7cbd3hlDAKnaykZeA1nK9EpqnqVNwu9WBYQjVNVU8t/Ld/Kq5/nctWlPXhyZipd2rfCXercUn4cct7yLBz4CaAQkeJZOHAKdI50u0LTingjIDJVNU1EfgXkq+qLXx7zdrEXwwLiwsxZk8ujb24hNiyEF+5Ip2+Pjm6X1PqcOAhbFjthUbABEGecImm6M7jd3saKjG95IyA+Bt4DvgdcCRzB6XJK8mahF8sC4sKt2VPEvXMyqa6p5YkZqYwZ2NPtklqvo7sge6ETFkW7nIUD+1/rzNy+dLwt8WF8whsBEQ7cCqxT1U9EJBYYraqverfUi2MB8e0cOFbGPa+uZ1thKZNSIvnFDfH06GT7ObtGFQ5ucoIiexGUHoQ2HWHgDU7Lou9oCLQuQeMdXllqQ0R6AUM9L9eq6mEv1ec1FhDfXkVVDU+t2s3Tq3bRPjiQR8YPYsbQGNvS1G21Nc4SH5sXwNalUFECIWGQMNkJi+hhEGAr95pvzxstiJuBPwGrAMHpZvqpqi5s4J6XgAnAYVVNrOP8T4FZnpdBwCCgh6oeE5F9QClQA1TXV/y5LCAu3q7DJ/n5ks2s2XuMIb1D+f3kJFvwr7moPg27PnLCYvu7znpQ7bo6E/L6XOmMXfRMsMAwF8QbAbEJuObLVoOI9AA+VNXBDdwzCjgJvFpXQJxz7UTgR6r6Hc/rfUC6qh49b3FnsYDwDlVlYUYev38nh9KKau4Z1ZcffKe/LdPRnJwuddZ+2rMK9n3qrDQLzqB275EQ92VgxFtgmAY1FBCN3e0+4JwupSLOs9Cfqq4WkbhGvv9MYG4jrzU+JiJMT49h7KBe/P6dHJ5etZvlWQX816RERg+wQexmoW0nZ/5E0jTndUmeExT7PnH+u225c/zLwOgzygmMHoMsMEyjNbYF8SecORBf/iN+C5Clqg+f5744YHlDLQgRCQHygEtU9Zjn2F7gOKDAs6r6XAP3zwZmA8TGxg7JzW1Wc/f8wue7i/j50s3sOXKKCckR/GpCPD07t3O7LNOQ4v2w71+e0FjtvAZn/OLsFkaPgRYYrZy3BqmnAiNxxiBWq+qSRtwTx/kD4hbgNlWdeNaxSFUtEJGewAfAg6q6+nyfZ11MvnO6uoZnVu3hbyt30TY4gIfHDeTWYbE2iN1SHM+F3H85E/P2fQIlB5zjIWFOUJwdGLb8R6vi2oZBjQyIJcACVX29nvO/Bk6q6p/P93kWEL63+8hJfrEkm8/3FJEa25XfT05iUERnt8syF+p47lddUns/gRN5zvGQ7k53VOpt0O87FhatwLcOCBEpxenm+cYpQFW1wX8ZzhcQItIF2AvEqOopz7EOOGMepZ7vPwB+q6rvNfRZYAHRVFSVJRvy+e+3cygpr+LuK/vww7H9CWnT2CEt06yoQrEnMPZ+4uzDXXYUwvrDsNmQMtMZ8zB+yZUWhIjMBUYD3YFDwKNAMICqPuO55k5gnKrOOOu+vsCX3VdBwOuq+rvGfKYFRNM6fqqSP7ybw/z1eUSHtue/JiXaTGx/UH3a2VZ17bOQn+EsV55yqxMW3S9xuzrjZbYntfGpNXuK+NmSzew+coobkiL41cR4etkgtn/Iy3CCInsx1FZBv7Ew/P/AJdfY4LafsIAwPne6uobnPt7D/67cRdvAAH46bgCzhve2pcT9xcnDkPF3WPcinCyE0D4w7B5ImQXtu7pdnbkIFhCmyew7eopfLM3m011HGRzTld9PTiQhsovbZRlvqalyNj9a8xwc+AKCO8DgGU73U8+BbldnvgULCNOkVJU3NxbwX8u3UlxexS1DY7hvdD+iQ201Ur9SsBHWPu8s/VFzGvpc5XQ/XTrOtlVtQSwgjCuKyyp57IMdzF3rTNKaNiSG+8dYUPidU0WQ+XdY95LzuGzXWBh6N6TeDiHd3K7OnIcFhHFVfnE5T6/axfx1eShqQeGvaqph+9tO91PupxDUHpKnw7D/A+ENLsdmXGQBYZqFguJynl61mzfWHaBWlenp0dw3+hJiullQ+J3CbFj7HGTNd1ad7T3SaVUMvAGCbK+R5sQCwjQrB0ucoJi31gmKaUOiuX+MBYVfKjsGG/4B65531oNq1xUSpzpPP0Wl2UztZsACwjRLFhStSG0N7P0YNr4OOcudVkX3S2HwTOcpqM6RblfYallAmGbtYEk5z6zazVxPUExNi+aB71hQ+K2KE87ueBvnwv7PAIF+Y2DwrU4XlO293aQsIEyLUFhSwTMf7+b1tfuprVWmpEXxwJj+xIbZPxh+69ge2DTPCYuS/c6yHgk3OV1QsSOsC6oJWECYFuXsoKipVaZaUPi/2lpnOfJNc511oKpOObO1v+yCCu3tdoV+ywLCtEiHTlTw9KqvgmJKahQPfOcSeod1cLs040unT0LOW7DpdWd1WdTZr2LwTIifBG07ul2hX7GAMC3aoROeFsWa/VRbULQuxQcga54zuH1sDwSHwKAbndVl4660BQO9wALC+IXDJyp45uM9zFmTS1VNLdfGh3PH5b25rG8YYn3V/k0VDqx1WhXZS+B0CXSJgeRbnLAI6+d2hS2WBYTxK4dPVPDiv/byxroDFJdV0b9nR+64PI4pqVF0aGubFvm9qnLY9rYzXrH7n6C1zjpQ6XfBgBsgqI3bFbYoFhDGL1VU1fDWpgJe+Xwf2fkn6NQ2iKlDorn9st7062H91K3CiQLYMAcyX3H22e7QE1JnQdp3oVsft6trESwgjF9TVTYcKObVz/bx9uaDVNUoV/bvzncvi2PMwJ62J0VrUFsDuz6CjJdhx3tOl1S/MZD+PWd12cBgtytstiwgTKtxpPQ089buZ86a/RSeqCA6tD23jejNLekxhHawrodWoSQPMl+DzFehtAA6hkPa7ZB2h7PSrPkaCwjT6lTV1PLB1kO88tk+1uw9RtugAG4cHMl3L48jMco2MGoVaqph5wqnVbHzA+dY/2tgyF3Q/1oItPEqsIAwrdy2whO8+nkuSzLzKa+qIS22K9+9PI7xiRG0CbLHJFuF4v1OiyLzNWfL1E6RTosi7Q7oEuV2da5yJSBE5CVgAnBYVb+xGLyIjAbeBPZ6Di1W1d96zo0DngACgRdU9Y+N+UwLCNOQkvIqFmbk8drn+9hXVEb3jm25dVgMtw7vTXiXdm6XZ5pCTZUzRrH+ZecJKBHof53zBNQlV7fKnfDcCohRwEng1QYC4ieqOuGc44HADuAaIA9YB8xU1a3n+0wLCNMYtbXK6p1HePXzXFZuP0yACOMSwrnjst4M69PN5lS0Fsf3QcYrznLkpw478yrS7nB2wusc4XZ1Tca1LiYRiQOWX2BAXAb8WlWv87z+TwBV/cP5Ps8Cwlyo/UVl/GNNLm+sO0BJeRUDenVi5rAYJqdG0yXEnnxpFaornZ3w1r/sLEkugTBgPAy7x5lf4ec/MDTngFiE00oowAmLLSIyDRinqnd7rrsdGK6qD9TzGbOB2QCxsbFDcnNzffArMf6uvLKGNzfmM2fNfjbnl9A2KIDrkyKYMTTGWhWtSdFuyPg7bJwDZUXQMx5G3AtJ0yG4vdvV+URzDYjOQK2qnhSR64EnVLW/iEwHrjsnIIap6oPn+zxrQRhvyM4vYd66/SzdUMDJ09X07dGBmUNjmZIWRVhH2y6zVaiqgOyF8MXTcCgb2ndz5lQMvdvvup+aZUDUce0+IB3oj3UxmWagrLKa5VkHmbd2P5n7iwkOFK5NCOfWYbFc1jeMAJuA5/9UYd8n8MUzsP0dZxA7YbLTqoga4nZ1XtEsA0JEwoFDqqoiMgxYCPTGeXJpBzAWyMcZpL5VVbec7/MsIIyvbC8sZe7a/SzZkE9JeRWx3UK4ZWgM09Oj6dnJnoBqFY7tgbXPO4/KVpZCzHAY/n1nddkWPKfCraeY5gKjge7AIeBRIBhAVZ8RkQeAe4FqoBz4d1X9zHPv9cDjOGHxkqr+rjGfaQFhfK2iqob3sgt5fe1+1u49RlCAMHZQT2YMi2VU/x62rEdrUHHCWX58zTNwfC90jnYGtNPugJBubld3wWyinDE+sPvISd5Yd4CFGXkcO1VJVNf2TE+P5ub0GCK7+ueApjlLbQ3seB/WPA17V0NQe0iZ6bQqegxwu7pGs4AwxodOV9fwwdZDzFt7gE93HSVAYPSAnswYGsN3BvYkKNBma/u9wmwnKLIWQM1p6DcWRtwH/b7T7Dc1soAwponsLyrjjfX7mb8+jyOlp+nZqS03p8dwc3qM7andGpw66synWPc8nDwEYf1hxPed7VLbNM8dEC0gjGliVTW1/HPbYeat3c+qHUdQheF9ujE9PYbrk8IJadNyBzVNI1RXwtal8MVTULAB2nVx9qgYNhu6xrhd3ddYQBjjovzichZn5LEwM4/cojI6tAnkhuQIpg2JYWhcqE3C82eqcGCNM58iZ5lzLO4KZ+LdoInQPtTd+rCAMKZZUFXW7TvOgvUHeHvzQcoqa4gLC2HakGimpEXbwLa/Kz7grCibvdB5ZDYg2Fl+PHGqs7SHS11QFhDGNDOnTlfzbnYhC9YfYM3eY4jAFZd0Z9qQaK5LCKddcOtbVbTVUHW6nbIXQfZiZ1Oj4A5OSCRNdwa2m3BfbQsIY5qx3KJTLMrMZ1FGHvnF5XRqF8TEwZFMHxJNSkxX64LyZ7W1sP8z2LzQGbMoPw7tukL8JEiaBr1H+nwJcgsIY1qA2lrliz1FLMjI493sg1RU1XJJz45OF1RqFD0724xtv1ZdCXtWOmGx7W2oOuVsl5o4BRKnQVSaT1aWtYAwpoU5UVHFO1kHWZCRR0bucQIErrq0B9PTYxg7qCdtg6wLyq9VljkbG2UvcrZNramE0D5OqyJxGvQc6LWPsoAwpgXbfeQkizLyWJyZT+GJCrqGBDNpcCTT02NIiOxsXVD+rrwYct5yBrf3rgathV6JzuB24lQI7X1Rb28BYYwfqKlVPtl5hIUZeazYeojK6loG9OrElLQoJqVE2baprUHpIWesYvNCyFvrHIse5rQs0r8HgRe+yZUFhDF+pqSsimVZBSzOzGPD/uIzT0FNTo3iuoRwOrS1iXh+73iu0wW1eaEzXvGDjd9qjMICwhg/tvfoKZZk5rFkYz4HjpUT0iaQcQnhTE6L4vJ+3W2F2dag7Ni3XknWAsKYVkBVWZ97nMWZeSzPOkhpRTW9OrflppQopqRFMyC8k9slmmbIAsKYVqaiqoaPcg6zZEMeq7YfobpWiY/ozJS0KG5MibRNjswZFhDGtGJFJ0/z1qYCFm/IJyuvhMAA4cr+znjFtfHhtG9jj8y2ZhYQxhgAdh0+yZINeSzJzKegpIKObYMYn+iMV4zoY/tst0YWEMaYr6mtVdbsPcbizDzezS7k5Olqorq2Z1JKJFPSorikp41XtBYWEMaYepVX1rBiayFLNuSzescRahWSorowOTWKiYMj6dGprdslGh+ygDDGNMrh0gqWbSxg6cZ8svNPEBggZ+ZXXJvQyzY68kMWEMaYC7bzUClLN+azdEMB+cVfza+4KTWKy/uF2V7bfsKVgBCRl4AJwGFVTazj/CzgYc/Lk8C9qrrJc24fUArUANX1FX8uCwhjvK+2Vlm37xhLN+afmV/Ro1NbbhwcyeTUKFsPqoVzKyBG4fzD/2o9AXE5kKOqx0VkPPBrVR3uObcPSFfVoxfymRYQxvhWRVUNK7cdZsmGfFZuP0xVjdK/Z0duSo1iUkok0aEhbpdoLpBrXUwiEgcsrysgzrkuFMhW1SjP631YQBjTrBWXVfL25oMsycxnfe5xAIb36cbk1CjGJ0XQpf2FLxxnml5LCIifAANV9W7P673AcUCBZ1X1uQbunQ3MBoiNjR2Sm5vrpeqNMY21v6iMNzfms2RDPnuOnqJNUABjB/ZkcmoUowf0pE2QjVc0V806IERkDPAUcIWqFnmORapqgYj0BD4AHlTV1ef7PGtBGOMuVSUrr4QlG/J5a1MBRacq6RoSzA1JEUxOjWJI71Abr2hmmm1AiEgysAQYr6o76rnm18BJVf3z+T7PAsKY5qOqppZPdx5lyYZ8VmwtpKKqlphu7ZmcEsVNqVH07dHR7RINDQeEaw81i0gssBi4/exwEJEOQICqlnq+vxb4rUtlGmO+peDAAMYM7MmYgT05ebqa97OdyXj/u3IXT/5zF4NjujI5JZKJgyMJ62iT8ZojXz7FNBcYDXQHDgGPAsEAqvqMiLwATAW+HDSoVtV0EemL06oAJ8BeV9XfNeYzrQVhTPNXWFLBsk35LNlQQM5BZzLeVZf2YHJqFNfE96JdsC0e2JRsopwxplnaVniCJRvyeXNDAYUnnMUDxyWGMyU1iuF9w2yzoyZgAWGMadZqapU1e4pYsiH/zOKB4Z3bMSnVmYw3MLyz2yX6LQsIY0yLUVFVwwdbD7F0Qz4f73A2OxoU0ZnJqZFMSomiV2fb7MibLCCMMS1S0cnTLM86yJIN+Ww8UIwIjOzXnZtSoxiXGE7HtrZ44MWygDDGtHh7jpxk6cYClm7IZ/+xMtoFB3BtfDg3pUZyxSU9bDLet2QBYYzxG6pK5v7jLNngLB5YXFZFl/bBXJfQixuSI7m8XxjBttJso1lAGGP8UmV1LZ/uOsLyTQdZsfUQJ09XExoSzLjECCYmR9iTUI1gAWGM8XsVVTV8vOMIy7MO8lHOIcoqa+jesS3XJ4UzITmS9N6htud2HSwgjDGtSnllDf/cdpi3NxfwUc5hTlfXEt65HdcnRTBhcASpMV1tTSgPCwhjTKt16nQ1H+YcYnnWQT7efoTKmlqiurZnQnIENyRHkBTVpVWHhQWEMcYAJyqq+GDLIZZnFfDJzqNU1yq9w0K4ISmCCcmRDIro1OrCwgLCGGPOUVxWyftbClmedZDPdhdRU6v07dGBCcmRTEyOoH+vTm6X2CQsIIwxpgFFJ0/zbnYhy7MKWLP3GKowoFenM91Q/rw0uQWEMcY00uETFbyz+SDLsw6e2Uo1PqIzEwZHMCEpktgw/9p32wLCGGO+hYMl5byd5YTFxgPFACRHd/G0LCKJ6tre3QK9wALCGGMu0oFjZWdaFpvzSwBIje3KhORIbkiKILxLy1xE0ALCGGO8KLfoFMs9LYucgycQgaG9u3FDcgTjk8Lp2anlhIUFhDHG+MjuIyc93VAF7Dh0kgCB4X3CmDA4gnEJ4c1+O1ULCGOMaQI7DpWyfFMBy7MOsufoKQIDhMv7hTEhOYLrEsLpGtLG7RK/wQLCGGOakKqSc7CU5VlOWOw/VkZQgHBF/+6MSwjn6vhedG8mLQsLCGOMcYmqkp1/guVZBby9+SB5x8sJEEjv3Y1rE3pxXUI4Md3ce3TWlYAQkZeACcBhVU2s47wATwDXA2XAnaqa6Tk3znMuEHhBVf/YmM+0gDDGNGdftize31LI+1sK2VZYCsCgiM5cl9CLa+PDm3y5D7cCYhRwEni1noC4HngQJyCGA0+o6nARCQR2ANcAecA6YKaqbj3fZ1pAGGNakv1FZazY6oTF+tzjqEJstxCuje/FdYnhpMWG+nw/i4YCwmcbuqrqahGJa+CSSTjhocAXItJVRCKAOGCXqu4BEJF5nmvPGxDGGNOSxIaFcPeVfbn7yr4cKT3NhzmHWLGlkFc/z+WFT/fSvWMbrh7kdENdfkkYbYMCm7Q+N3f8jgIOnPU6z3OsruPD63sTEZkNzAaIjY31fpXGGNMEenRqy8xhscwcFktpRRWrth85s5jgvHUH6NAmkNEDe3JdQjhjBvSgU7tgn9fkZkDU1W7SBo7XSVWfA54Dp4vJO6UZY4x7OrULZuLgSCYOjuR0dQ2f7S5ixZZCPth6iLezDtImMIDLLwnjuoRwrh7Uix6dfPNElJsBkQfEnPU6GigA2tRz3BhjWp22QYGMGdCTMQN68t83KZn7j7NiSyHvbznEfy7ezM9kM0PjujHn7uEEBwZ49bPdDIhlwAOeMYbhQImqHhSRI0B/EekD5AMzgFtdrNMYY5qFwABhaFw3hsZ142fXD2JbofNE1KETFV4PB/BhQIjIXGA00F1E8oBHgWAAVX0GeAfnCaZdOI+53uU5Vy0iDwDv4zzm+pKqbvFVncYY0xKJCIMiOjMoorPPPsOXTzHNPM95Be6v59w7OAFijDHGJd5vkxhjjPELFhDGGGPqZAFhjDGmThYQxhhj6mQBYYwxpk4WEMYYY+pkAWGMMaZOfrVhkGcWdu63vL07cNSL5fhSS6oVWla9LalWaFn1tqRaoWXVezG19lbVHnWd8KuAuBgisr6+NdGbm5ZUK7SseltSrdCy6m1JtULLqtdXtVoXkzHGmDpZQBhjjKmTBcRXnnO7gAvQkmqFllVvS6oVWla9LalWaFn1+qRWG4MwxhhTJ2tBGGOMqZMFhDHGmDq1+oAQkXEisl1EdonII27X0xARiRGRlSKSIyJbROSHbtd0PiISKCIbRGS527Wcj4h0FZGFIrLN83t8mds11UdEfuT5M5AtInNFpJ3bNZ1NRF4SkcMikn3WsW4i8oGI7PT8N9TNGr9UT61/8vw5yBKRJSLS1cUSv6aues869xMRURHp7o3PatUBISKBwN+A8UA8MFNE4t2tqkHVwI9VdRAwAri/mdcL8EMgx+0iGukJ4D1VHQgMppnWLSJRwA+AdFVNxNl5cYa7VX3D34Fx5xx7BPhIVfsDH3leNwd/55u1fgAkqmoysAP4z6YuqgF/55v1IiIxwDXAfm99UKsOCGAYsEtV96hqJTAPmORyTfVS1YOqmun5vhTnH7Aod6uqn4hEAzcAL7hdy/mISGdgFPAigKpWqmqxq0U1LAhoLyJBQAhQ4HI9X6Oqq4Fj5xyeBLzi+f4V4KamrKk+ddWqqitUtdrz8gsguskLq0c9v7cA/wP8B+C1J49ae0BEAQfOep1HM/4H92wiEgekAmtcLqUhj+P8ga11uY7G6AscAV72dIm9ICId3C6qLqqaD/wZ5yfFg0CJqq5wt6pG6aWqB8H5YQfo6XI9jfU94F23i2iIiNwI5KvqJm++b2sPCKnjWLN/7ldEOgKLgIdU9YTb9dRFRCYAh1U1w+1aGikISAOeVtVU4BTNpwvkazx995OAPkAk0EFEbnO3Kv8kIj/H6dqd43Yt9RGREODnwK+8/d6tPSDygJizXkfTzJrq5xKRYJxwmKOqi92upwEjgRtFZB9O1913ROQf7pbUoDwgT1W/bJEtxAmM5uhqYK+qHlHVKmAxcLnLNTXGIRGJAPD897DL9TRIRL4LTABmafOeMNYP54eFTZ6/b9FApoiEX+wbt/aAWAf0F5E+ItIGZ6Bvmcs11UtEBKePPEdVH3O7noao6n+qarSqxuH8vv5TVZvtT7mqWggcEJEBnkNjga0ultSQ/cAIEQnx/JkYSzMdUD/HMuC7nu+/C7zpYi0NEpFxwMPAjapa5nY9DVHVzaraU1XjPH/f8oA0z5/pi9KqA8IzCPUA8D7OX7D5qrrF3aoaNBK4Heen8Y2er+vdLsqPPAjMEZEsIAX4vbvl1M3TylkIZAKbcf4eN6tlIURkLvA5MEBE8kTk34A/AteIyE6cp23+6GaNX6qn1r8CnYAPPH/PnnG1yLPUU69vPqt5t5yMMca4pVW3IIwxxtTPAsIYY0ydLCCMMcbUyQLCGGNMnSwgjDHG1MkCwphmQERGt4QVb03rYgFhjDGmThYQxlwAEblNRNZ6Jk8969nv4qSI/EVEMkXkIxHp4bk2RUS+OGtPgVDP8UtE5EMR2eS5p5/n7TuetR/FHM8saWNcYwFhTCOJyCDgFmCkqqYANcAsoAOQqappwMfAo55bXgUe9uwpsPms43OAv6nqYJw1lA56jqcCD+HsTdIXZ+a8Ma4JcrsAY1qQscAQYJ3nh/v2OAvO1QJveK75B7BYRLoAXVX1Y8/xV4AFItIJiFLVJQCqWgHgeb+1qprneb0RiAM+9fmvyph6WEAY03gCvKKqX9tdTER+ec51Da1f01C30emzvq/B/n4al1kXkzGN9xEwTUR6wpk9lnvj/D2a5rnmVuBTVS0BjovIlZ7jtwMfe/bvyBORmzzv0daznr8xzY79hGJMI6nqVhH5BbBCRAKAKuB+nM2FEkQkAyjBGacAZ0nrZzwBsAe4y3P8duBZEfmt5z2mN+Evw5hGs9VcjblIInJSVTu6XYcx3mZdTMYYY+pkLQhjjDF1shaEMcaYOllAGGOMqZMFhDHGmDpZQBhjjKmTBYQxxpg6/X8RBc0NSsAzTQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(val_losses, label=\"validation\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vDPz0G4MfyL6",
    "outputId": "b7e93030-f301-44ea-e6ba-5d917286e439"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss 1.3500278586 | test ppl 3.8575329947\n",
      "=========================================================================================\n",
      "Last validation loss: 1.35645076053176\n"
     ]
    }
   ],
   "source": [
    "eval_model = best_model\n",
    "test_loss = evaluate(best_model, test_src, test_tgt)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.10f} | test ppl {:8.10f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)\n",
    "\n",
    "print(\"Last validation loss: {}\".format(val_losses[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ZsUG0R1oXkTT"
   },
   "outputs": [],
   "source": [
    "idx_to_letter = {val:key for key, val in char_nums.items()}\n",
    "\n",
    "def sample_categorical(lnprobs, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Sample an element from a categorical distribution\n",
    "    :param lnprobs: Outcome log-probabilities\n",
    "    :param temperature: Sampling temperature. 1.0 follows the given distribution,\n",
    "        0.0 returns the maximum probability element.\n",
    "    :return: The index of the sampled element.\n",
    "    \"\"\"\n",
    "\n",
    "    if temperature == 0.0:\n",
    "        return lnprobs.argmax()\n",
    "    p = F.softmax(lnprobs / temperature, dim=1)\n",
    "\n",
    "    #print(\"softmaxed probs:\", p)\n",
    "    \n",
    "    return dist.Categorical(p).sample()\n",
    "\n",
    "def sample_sentence(model, query, max_len = 140, temperature=1):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    while len(query) < max_len and '<eos>' not in query:\n",
    "        data, targets = batchify([query])\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "    \n",
    "        src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "        src_pad_mask = (data == 0).bool().view(data.size(1), data.size(0))\n",
    "        \n",
    "        tgt_mask = model.generate_square_subsequent_mask(targets.size(0)).to(device)\n",
    "        tgt_pad_mask = (targets == 0).bool().view(targets.size(1), targets.size(0))   \n",
    "        \n",
    "        output = output = eval_model(data, src_mask, targets, tgt_mask, src_pad_mask, tgt_pad_mask).view(-1, ntokens)\n",
    "        \n",
    "        next_char_idx = sample_categorical(output, temperature) #0.5\n",
    "                \n",
    "        try:\n",
    "            query += [idx_to_letter[int(next_char_idx[-1])]]\n",
    "        except IndexError:\n",
    "            query += [idx_to_letter[int(next_char_idx)]]\n",
    "            \n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N3rHcGmkG3_m",
    "outputId": "45ef1fc7-ae43-48ac-dc0a-f5382bd3d713"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478 <sos> E A W P V G V C C C A A C C C A A C C A A G C R C F V S I C C L A E C A A S C F A A C C C L L C S V A C C V A W C V A I C C V V C C L A E C V S C S V Y C C C C C A L C R A A C C A A A C C C L C C A V C C A A C C A A G Y R C C L A C C C L A E A I C C C A F C F A F C S A V C W A I C C A A G C S A R C A A C A A G C R A A S <sos> R G V E C P V G C E A A C C R R S C F A F F P S V C C A V C C A Q C P V P C R C C A A A C C A V C C C A A C I A C C A A A A C C A A C P A L C C L L Y Y C A A A C C A A C A C C A A C C L A Y C C V V C C A I A C C A A C A A A C S A A A C C R A C C V A V C I L K C C A L C E A C C A A G C C A A L C R C H V V C C A R C C V A V C C I A C C A A C A A C V A V V C I R C G A K C A A C C A A A G C C L A M C G A F C F A S C L L C E L F C L A F C A A C G A F C A A A C C A A C C A A C C A A C C V C C H A W C V A V C I <sos> C G A C C A A S S V R C C A C A T C I A C C L V V P C A V C C C A A C C C F A S A C C C S A C C A A C A A G P W G R <eos>\n",
      "100 <sos> E G V C C A A C C A A C C A A A A C C C A A A G C C A L A Y C C A L S V P C F C L A R C F A S C D A A P S G C C C L L P C G A C C A A C G L R C A R C A A C C A C F L S C R V V C C A A C C A A G C <eos>\n",
      "150 <sos> E A A C C A A C P A G C R A C C A A G G R R A F C F C S A C C V A C A L C S A <sos> C A A S C C A C C C C C C L A R C A A A K C S L L V L C L A C C C A A C C V A I C C C C A C C A A A A C C A A A S A C C A A C C C V A C C T A P G G R C C A A C C A A C C C A A I C C V R C A A C C C S A C C C A A G C <eos>\n",
      "54 <sos> E G W K K Y P C L A R S C T L F M F H A W C V L I Y C A A A C C A A G A N C F A S C L E V I S C V C G C <eos>\n",
      "260 <sos> E G <sos> <sos> G A R C C L L L V Y C C A V C C A L C E A A S H A W C R L T Q F A P C P C G A R C A S G C C C A A C C A A C C A A C C A A C C C Y C C L A G C C R V I C C V A C C V A I C C A A C C C A A A S I L C C A A S C C A A C C A A C C C C V C I F C S V R C A C C C A L C V C C C A A G C C R L C V A C S A F C R A A C C V A V G C R T A F C V A C C A A C C A A C C V V I I C C A V C V A C C A L C E A A C C A A C C V A C C A A C C C S A P C G I L C C V C C A A S C L A E C A A C C A A C S A E S H L C C A A C C C A V G C <eos>\n"
     ]
    }
   ],
   "source": [
    "import torch.distributions as dist\n",
    "for _ in range(5):\n",
    "    sample = sample_sentence(model, [\"<sos>\", \"E\"], max_len = 500, temperature=0.9)\n",
    "\n",
    "    print(len(sample), \" \".join(sample))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VvvgjJGWHhB5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9d0EO5DHXkTT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "project2_jonas.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
