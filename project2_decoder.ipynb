{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "el6WZUx3ctwE",
    "outputId": "f66a38ff-d81d-4b57-d8b2-a43989ee3151"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from google.colab import drive\\ndrive.mount('/content/drive')\\n!ls drive/'My Drive/YYY_deep_project_YYY'\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\"\"\"from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!ls drive/'My Drive/YYY_deep_project_YYY'\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nUc-3FwsYA6X",
    "outputId": "0e1b95cc-1151-4102-ccea-a9e0edbe1f5d"
   },
   "outputs": [],
   "source": [
    "#%cd drive/'My Drive/YYY_deep_project_YYY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OcYm_Z9sdVq4"
   },
   "outputs": [],
   "source": [
    "def get_sequence(infile):\n",
    "\n",
    "    while True:\n",
    "\n",
    "        header = infile.readline()\n",
    "        sequence = infile.readline()\n",
    "\n",
    "        pdb = header[1:5]\n",
    "\n",
    "        if not header or not sequence or set(sequence) == {'X'}:\n",
    "            return\n",
    "        \n",
    "        yield header.strip()[1:], sequence.strip(), pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "mzJSiJbpXkTR",
    "outputId": "2af42cfb-ef10-4316-de19-6b89fa008527"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sequences = []\n",
    "seq_to_pdb = {}\n",
    "count = 0\n",
    "with open('data/sra100k_done.fa') as infile:\n",
    "#with open('data/all_heavy.fasta') as infile:\n",
    "        for header, sequence, pdb in get_sequence(infile):\n",
    "            sequences.append(list(sequence))\n",
    "            seq_to_pdb[sequence] = pdb\n",
    "\n",
    "#sequences = list(filter(lambda x:len(x)<150, sequences))\n",
    "\n",
    "import random\n",
    "random.shuffle(sequences)\n",
    "\n",
    "sequences = sequences[:5000]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "lengths = [len(seq) for seq in sequences]\n",
    "\n",
    "print(len(sequences))\n",
    "\n",
    "plt.hist(lengths, bins=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lVSriKBHdqU7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 500 500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4000, 500, 500)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils import data\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the size of the dataset\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve inputs and targets at the given index\n",
    "        X = self.inputs[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "def create_datasets(sequences, dataset_class, p_train=0.8, p_val=0.1, p_test=0.1):\n",
    "    \n",
    "    \n",
    "    # Define partition sizes\n",
    "    num_train = int(len(sequences)*p_train)\n",
    "    num_val = int(len(sequences)*p_val)\n",
    "    num_test = int(len(sequences)*p_test)\n",
    "    \n",
    "    print(num_train, num_val, num_test)\n",
    "\n",
    "    # Split sequences into partitions\n",
    "    sequences_train = sequences[:num_train]\n",
    "\n",
    "    # add reversed sequences for training and shuffle\n",
    "    #sequences_train += [seq[::-1] for seq in sequences_train]\n",
    "    #shuffle(sequences_train)\n",
    "\n",
    "    sequences_val = sequences[num_train:num_train+num_val]\n",
    "    sequences_test = sequences[num_train+num_val:]\n",
    "\n",
    "    input_train = [['<sos>'] + list(seq)+['<eos>'] for seq in sequences_train]\n",
    "    input_val = [['<sos>'] + list(seq)+['<eos>'] for seq in sequences_val]\n",
    "    input_test = [['<sos>'] + list(seq)+['<eos>'] for seq in sequences_test]\n",
    "\n",
    "    return (input_train, input_val, input_test)\n",
    "\n",
    "(input_train, input_val, input_test) = create_datasets(sequences, Dataset)\n",
    "\n",
    "len(input_train), len(input_val), len(input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YNWaI923d3YE"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "        \n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        \n",
    "        self.t_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        \n",
    "        self.embed = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        \n",
    "        decoder_layers = TransformerDecoderLayer(ninp, nhead, nhid, dropout)\n",
    "        \n",
    "        self.t_decoder = TransformerDecoder(decoder_layers, nlayers)\n",
    "\n",
    "        \n",
    "        #self.ff = nn.Linear(ninp, ntoken)\n",
    "        \n",
    "        self._init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float(-1e-10)).masked_fill(mask == 1, float(0.0))\n",
    "        \n",
    "        return mask\n",
    "\n",
    " \n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "       \n",
    "\n",
    "    def forward(self, src, src_mask, tgt, tgt_mask, src_pad_mask, tgt_pad_mask, mem_pad_mask=None):\n",
    "        \n",
    "\n",
    "        embeds = self.embed(src) * math.sqrt(self.ninp)\n",
    "        \n",
    "        positions = self.pos_encoder(embeds)\n",
    "    \n",
    "        \n",
    "        encoded = self.t_encoder(positions)\n",
    "        #encoded = self.t_encoder(positions, src_key_padding_mask=src_pad_mask)\n",
    "        \n",
    "        \n",
    "        if mem_pad_mask is None:\n",
    "            mem_pad_mask = tgt_pad_mask.clone()\n",
    "            \n",
    "        embeds = self.embed(tgt) * math.sqrt(self.ninp)\n",
    "        \n",
    "        positions = self.pos_encoder(embeds)\n",
    "        \n",
    "        #decoded = self.t_decoder(tgt=positions, memory=encoded, tgt_mask=tgt_mask,\n",
    "        #                         tgt_key_padding_mask=tgt_pad_mask,\n",
    "        #                         memory_key_padding_mask=mem_pad_mask)\n",
    "        \n",
    "        decoded = self.t_decoder(tgt=positions, memory=encoded, tgt_mask=tgt_mask)\n",
    "  \n",
    "        #output = self.ff(decoded)\n",
    "\n",
    "        return decoded\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qBtnWGt-Al99",
    "outputId": "50cdbe94-9ab1-4a92-a5e6-5f50335a9208"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4000, 141])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "vocab = ['<pad>', \"<sos>\", \"<eos>\"] + [\"A\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"K\",\"L\",\"M\",\"N\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"V\",\"W\",\"X\",\"Y\"]\n",
    "char_nums = {token:vocab.index(token) for token in vocab}\n",
    "\n",
    "\n",
    "def batchify(data):\n",
    "    \n",
    "    # get the length of each seq in your batch\n",
    "    seq_lengths = torch.LongTensor([len(seq) for seq in data])\n",
    "\n",
    "    train_vectorized = [[char_nums[char] for char in seq] for seq in data]\n",
    "    sources = [seq[:-1] for seq in train_vectorized]\n",
    "    targets = [seq[1:] for seq in train_vectorized]\n",
    "    \n",
    "    # dump padding everywhere, and place seqs on the left.\n",
    "    # NOTE: you only need a tensor as big as your longest sequence\n",
    "    src_tensor = torch.zeros((len(train_vectorized), seq_lengths.max()-1)).long()\n",
    "    tgt_tensor = torch.zeros((len(train_vectorized), seq_lengths.max()-1)).long()\n",
    "\n",
    "    \n",
    "    for idx, (seq, seqlen) in enumerate(zip(sources, seq_lengths)):\n",
    "        src_tensor[idx, :(seqlen-1)] = torch.LongTensor(seq)\n",
    "    \n",
    "    for idx, (seq, seqlen) in enumerate(zip(targets, seq_lengths)):\n",
    "        tgt_tensor[idx, :(seqlen-1)] = torch.LongTensor(seq)\n",
    "        \n",
    "        \n",
    "    # SORT YOUR TENSORS BY LENGTH!\n",
    "    seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "    \n",
    "    src_tensor = src_tensor[perm_idx]\n",
    "    tgt_tensor = tgt_tensor[perm_idx]\n",
    "\n",
    "    return src_tensor, tgt_tensor\n",
    "\n",
    "\n",
    "train_src, train_tgt = batchify(input_train)\n",
    "val_src, val_tgt = batchify(input_val)\n",
    "test_src, test_tgt = batchify(input_test)\n",
    "\n",
    "train_src.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "UglAQA33frSw"
   },
   "outputs": [],
   "source": [
    "bptt = 100\n",
    "\n",
    "def get_batch(sources, targets, i):\n",
    "    n_seqs = min(bptt, len(sources) - 1 - i)\n",
    "    \n",
    "    max_len = max([len(seq) for seq in sources])\n",
    "    \n",
    "    src = torch.cat([sources[i] for i in range(n_seqs)]).view(n_seqs,max_len)\n",
    "        \n",
    "    target = torch.cat([targets[i] for i in range(n_seqs)]).view(n_seqs,max_len)#.reshape(-1)\n",
    "    \n",
    "    return src, target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tLXVBEIrfr-D"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1128"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntokens = len(vocab) # the size of vocabulary\n",
    "nhid = ntokens # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 6  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 8 # the number of heads in the multiheadattention models\n",
    "emsize = train_src.shape[1]*nhead # embedding dimension, must be divisible by nheads\n",
    "dropout = 0.5 # the dropout value\n",
    "\n",
    "emsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1,  8,  6,  ..., 20, 16,  8],\n",
       "         [ 1,  8,  6,  ...,  8,  4, 12],\n",
       "         [ 1,  8,  6,  ...,  8,  4, 12],\n",
       "         ...,\n",
       "         [ 1,  3, 18,  ...,  0,  0,  0],\n",
       "         [ 1,  8,  6,  ...,  0,  0,  0],\n",
       "         [ 1,  3, 18,  ...,  0,  0,  0]]),\n",
       " tensor([[ 8,  6, 18,  ..., 16,  8,  2],\n",
       "         [ 8,  6, 18,  ...,  4, 12,  2],\n",
       "         [ 8,  6, 18,  ...,  4, 12,  2],\n",
       "         ...,\n",
       "         [ 3, 18, 20,  ...,  0,  0,  0],\n",
       "         [ 8,  6, 18,  ...,  0,  0,  0],\n",
       "         [ 3, 18, 20,  ...,  0,  0,  0]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_src, train_tgt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Qc1m7dgifuPc"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#  ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lr = 0.1 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "#optimizer  = torch.optim.Adam(lr=lr, params=model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.8)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "model.to(device)\n",
    "import sys\n",
    "import time\n",
    "def train():\n",
    "    \n",
    "    train_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(vocab)\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    \n",
    "    for batch, i in enumerate(range(0, train_src.size(0) - 1, bptt)):\n",
    "        \n",
    "        data, targets = get_batch(train_src, train_tgt, i)\n",
    "    \n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        \n",
    "        targets = targets.view(data.shape)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "            \n",
    "        src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "        src_pad_mask = (data == 0).bool().view(data.size(1), data.size(0))\n",
    "        \n",
    "        tgt_mask = model.generate_square_subsequent_mask(targets.size(0)).to(device)\n",
    "        tgt_pad_mask = (targets == 0).bool().view(targets.size(1), targets.size(0))\n",
    "        \n",
    "        output = model(data, src_mask, targets, tgt_mask, src_pad_mask, tgt_pad_mask)\n",
    "     \n",
    "        output_trans = output.view(-1, ntokens)\n",
    "        #target_trans = targets.view(-1, targets.size(0) * targets.size(1)).squeeze(0)\n",
    "        target_trans = targets.view(-1)\n",
    "            \n",
    "        loss = criterion(output_trans, target_trans)\n",
    "        \n",
    "        n_batches += 1\n",
    "        train_loss += loss\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.4)\n",
    "        \n",
    "        # Soft, hard accuracy\n",
    "        #o = list(output.view(-1, ntokens)[0])\n",
    "        #t = targets\n",
    "        #print(o,t)\n",
    "        #hard_acc = sum([i for i in range(len(targets)) if o[i] == t[i]])/len(targets)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        log_interval = 10\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}|'.format(\n",
    "                    epoch, batch, len(train_src) // bptt, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "    \n",
    "    train_losses.append(train_loss/n_batches)\n",
    "\n",
    "def evaluate(eval_model, src, tgt):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, src.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(src, tgt, i)\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            \n",
    "            targets = targets.view(data.shape)\n",
    "            \n",
    "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            src_pad_mask = (data == 0).bool().view(data.size(1), data.size(0))\n",
    "        \n",
    "            tgt_mask = model.generate_square_subsequent_mask(targets.size(0)).to(device)\n",
    "            tgt_pad_mask = (targets == 0).bool().view(targets.size(1), targets.size(0))    \n",
    "            \n",
    "            output = eval_model(data, src_mask, targets, tgt_mask, src_pad_mask, tgt_pad_mask)\n",
    "            \n",
    "            output_trans = output.view(-1, ntokens)\n",
    "            #target_trans = targets.view(-1, targets.size(1) * targets.size(0)).squeeze(0)\n",
    "            target_trans = targets.view(-1)\n",
    "            \n",
    "            total_loss += len(data) * criterion(output_trans, target_trans).item()\n",
    "            \n",
    "    return total_loss / (len(src) - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "tZDramCJfwec",
    "outputId": "6b842e7f-b493-42a1-8118-c263cf475722"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (662700) to match target batch_size (14100).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-0d9c97193ea3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_tgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mval_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-cb1206b8566d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mtarget_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_trans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_trans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mn_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m    962\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2466\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2468\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2260\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2261\u001b[0;31m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0m\u001b[1;32m   2262\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   2263\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (662700) to match target batch_size (14100)."
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 1 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(model, val_src, val_tgt)\n",
    "    val_losses.append(val_loss)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "XPTZdip9XkTT"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(val_losses, label=\"validation\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vDPz0G4MfyL6",
    "outputId": "b7e93030-f301-44ea-e6ba-5d917286e439"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss 1.3500278586 | test ppl 3.8575329947\n",
      "=========================================================================================\n",
      "Last validation loss: 1.35645076053176\n"
     ]
    }
   ],
   "source": [
    "eval_model = best_model\n",
    "test_loss = evaluate(best_model, test_src, test_tgt)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.10f} | test ppl {:8.10f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)\n",
    "\n",
    "print(\"Last validation loss: {}\".format(val_losses[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ZsUG0R1oXkTT"
   },
   "outputs": [],
   "source": [
    "idx_to_letter = {val:key for key, val in char_nums.items()}\n",
    "\n",
    "def sample_categorical(lnprobs, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Sample an element from a categorical distribution\n",
    "    :param lnprobs: Outcome log-probabilities\n",
    "    :param temperature: Sampling temperature. 1.0 follows the given distribution,\n",
    "        0.0 returns the maximum probability element.\n",
    "    :return: The index of the sampled element.\n",
    "    \"\"\"\n",
    "\n",
    "    if temperature == 0.0:\n",
    "        return lnprobs.argmax()\n",
    "    p = F.softmax(lnprobs / temperature, dim=1)\n",
    "\n",
    "    #print(\"softmaxed probs:\", p)\n",
    "    \n",
    "    return dist.Categorical(p).sample()\n",
    "\n",
    "def sample_sentence(model, query, max_len = 140, temperature=1):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    while len(query) < max_len and '<eos>' not in query:\n",
    "        data, targets = batchify([query])\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "    \n",
    "        src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "        src_pad_mask = (data == 0).bool().view(data.size(1), data.size(0))\n",
    "        \n",
    "        tgt_mask = model.generate_square_subsequent_mask(targets.size(0)).to(device)\n",
    "        tgt_pad_mask = (targets == 0).bool().view(targets.size(1), targets.size(0))   \n",
    "        \n",
    "        output = output = eval_model(data, src_mask, targets, tgt_mask, src_pad_mask, tgt_pad_mask).view(-1, ntokens)\n",
    "        \n",
    "        next_char_idx = sample_categorical(output, temperature) #0.5\n",
    "                \n",
    "        try:\n",
    "            query += [idx_to_letter[int(next_char_idx[-1])]]\n",
    "        except IndexError:\n",
    "            query += [idx_to_letter[int(next_char_idx)]]\n",
    "            \n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N3rHcGmkG3_m",
    "outputId": "45ef1fc7-ae43-48ac-dc0a-f5382bd3d713"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478 <sos> E A W P V G V C C C A A C C C A A C C A A G C R C F V S I C C L A E C A A S C F A A C C C L L C S V A C C V A W C V A I C C V V C C L A E C V S C S V Y C C C C C A L C R A A C C A A A C C C L C C A V C C A A C C A A G Y R C C L A C C C L A E A I C C C A F C F A F C S A V C W A I C C A A G C S A R C A A C A A G C R A A S <sos> R G V E C P V G C E A A C C R R S C F A F F P S V C C A V C C A Q C P V P C R C C A A A C C A V C C C A A C I A C C A A A A C C A A C P A L C C L L Y Y C A A A C C A A C A C C A A C C L A Y C C V V C C A I A C C A A C A A A C S A A A C C R A C C V A V C I L K C C A L C E A C C A A G C C A A L C R C H V V C C A R C C V A V C C I A C C A A C A A C V A V V C I R C G A K C A A C C A A A G C C L A M C G A F C F A S C L L C E L F C L A F C A A C G A F C A A A C C A A C C A A C C A A C C V C C H A W C V A V C I <sos> C G A C C A A S S V R C C A C A T C I A C C L V V P C A V C C C A A C C C F A S A C C C S A C C A A C A A G P W G R <eos>\n",
      "100 <sos> E G V C C A A C C A A C C A A A A C C C A A A G C C A L A Y C C A L S V P C F C L A R C F A S C D A A P S G C C C L L P C G A C C A A C G L R C A R C A A C C A C F L S C R V V C C A A C C A A G C <eos>\n",
      "150 <sos> E A A C C A A C P A G C R A C C A A G G R R A F C F C S A C C V A C A L C S A <sos> C A A S C C A C C C C C C L A R C A A A K C S L L V L C L A C C C A A C C V A I C C C C A C C A A A A C C A A A S A C C A A C C C V A C C T A P G G R C C A A C C A A C C C A A I C C V R C A A C C C S A C C C A A G C <eos>\n",
      "54 <sos> E G W K K Y P C L A R S C T L F M F H A W C V L I Y C A A A C C A A G A N C F A S C L E V I S C V C G C <eos>\n",
      "260 <sos> E G <sos> <sos> G A R C C L L L V Y C C A V C C A L C E A A S H A W C R L T Q F A P C P C G A R C A S G C C C A A C C A A C C A A C C A A C C C Y C C L A G C C R V I C C V A C C V A I C C A A C C C A A A S I L C C A A S C C A A C C A A C C C C V C I F C S V R C A C C C A L C V C C C A A G C C R L C V A C S A F C R A A C C V A V G C R T A F C V A C C A A C C A A C C V V I I C C A V C V A C C A L C E A A C C A A C C V A C C A A C C C S A P C G I L C C V C C A A S C L A E C A A C C A A C S A E S H L C C A A C C C A V G C <eos>\n"
     ]
    }
   ],
   "source": [
    "import torch.distributions as dist\n",
    "for _ in range(5):\n",
    "    sample = sample_sentence(model, [\"<sos>\", \"E\"], max_len = 500, temperature=0.9)\n",
    "\n",
    "    print(len(sample), \" \".join(sample))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VvvgjJGWHhB5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9d0EO5DHXkTT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "project2_jonas.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
